{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages needed for this notebook\n",
    "import gymnasium as gym\n",
    "import gym_RLcourse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    \n",
    "    def __init__(self, nA=4, nS=64):\n",
    "        self.nA = nA # Number of actions\n",
    "        self.nS = nS # Number of states\n",
    "        \n",
    "        # Uniform probabilites in each state.\n",
    "        # That is, in each of the nS states\n",
    "        # each of the nA actions has probability\n",
    "        # 1/nA.\n",
    "        self.probs = np.ones((nS,nA))/nA \n",
    "\n",
    "    def act(self, state):\n",
    "        action = np.random.choice(self.nA, p=self.probs[state]) \n",
    "        return action # a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(env, agent):\n",
    "    state, info = env.reset()\n",
    "    time_step = 0\n",
    "    total_reward = 0\n",
    "    truncated = False\n",
    "    terminated = False\n",
    "    while not truncated and not terminated:\n",
    "        action = agent.act(state);\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        time_step += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print(\"Time step:\", time_step)\n",
    "        print(\"State:\", state)\n",
    "        print(\"Action:\", action)\n",
    "        print(\"Total reward:\", total_reward)\n",
    "        \n",
    "    if truncated:\n",
    "        print(\"The environment was truncated even though a terminal state was not reached.\")\n",
    "    elif terminated:\n",
    "        print(\"A terminal state was reached.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Discrete(64)\n",
      "Action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v1', render_mode=\"human\")\n",
    "state, info = env.reset()\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "With probability 0.33 you will move to state 8 and get reward 0.0.\n",
      "With probability 0.33 you will move to state 1 and get reward 0.0.\n",
      "With probability 0.33 you will move to state 0 and get reward 0.0.\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "a = RIGHT \n",
    "print(env.unwrapped.P[s][a])\n",
    "for p, next_s, reward, _ in env.unwrapped.P[s][a]:\n",
    "    print(\"With probability %.2f you will move to state %d and get reward %.1f.\" % (p, next_s, reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step: 24\n",
      "State: 19\n",
      "Action: 2\n",
      "Total reward: 0.0\n",
      "A terminal state was reached.\n"
     ]
    }
   ],
   "source": [
    "agent = RandomAgent()\n",
    "run_agent(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_action_value(env, discount, s, a, v):\n",
    "    \n",
    "    action_value = 0\n",
    "    \n",
    "    # Loop through all possible (s', r) pairs\n",
    "    for p, next_s, reward, _ in env.unwrapped.P[s][a]:\n",
    "        action_value += p * (reward + discount*v[next_s]) \n",
    "    \n",
    "    return action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bellman_RHS(env, discount, agent, s, v):\n",
    "    \n",
    "    state_value = 0\n",
    "    \n",
    "    for a in range(env.action_space.n):\n",
    "        # Loop through all possible actions\n",
    "        state_value += agent.probs[s][a]*compute_action_value(env, discount, s, a, v) \n",
    "    \n",
    "    return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bellman_RHS_all(env, discount, agent, v0):\n",
    "    # v0 is the given value function\n",
    "    # v will be the right-hand side of the Bellman equation\n",
    "    # If v0 is indeed the value function, then we should get v = v0.\n",
    "    \n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for s in range(env.observation_space.n):\n",
    "        v[s] = Bellman_RHS(env, discount, agent, s, v0)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, discount, agent, v0, max_iter=10000, tol=1e-6):\n",
    "    \n",
    "    v_old = v0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        v_new = Bellman_RHS_all(env, discount, agent, v_old)\n",
    "        \n",
    "        if np.max(np.abs(v_new-v_old)) < tol:\n",
    "            break\n",
    "            \n",
    "        v_old = v_new\n",
    "        \n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.88195473e-03 2.14943930e-03 2.79687854e-03 4.10390763e-03\n",
      "  6.53056742e-03 9.78455641e-03 1.34273647e-02 1.59485493e-02]\n",
      " [1.61810514e-03 1.77291032e-03 2.14038650e-03 2.98712882e-03\n",
      "  5.70621601e-03 9.39901211e-03 1.45525909e-02 1.84735439e-02]\n",
      " [1.20269457e-03 1.18668645e-03 1.00712544e-03 0.00000000e+00\n",
      "  3.91044692e-03 7.55535622e-03 1.69135293e-02 2.49228339e-02]\n",
      " [8.05878684e-04 7.66260403e-04 7.02931249e-04 7.70972167e-04\n",
      "  2.38133947e-03 0.00000000e+00 2.06255200e-02 3.93839677e-02]\n",
      " [4.50526176e-04 3.71089481e-04 2.68385702e-04 0.00000000e+00\n",
      "  4.84438436e-03 1.15928695e-02 2.62057426e-02 7.26051893e-02]\n",
      " [1.75712022e-04 0.00000000e+00 0.00000000e+00 1.44822264e-03\n",
      "  5.40352770e-03 1.53215855e-02 0.00000000e+00 1.52226788e-01]\n",
      " [7.70749990e-05 0.00000000e+00 1.09331519e-04 3.89386428e-04\n",
      "  0.00000000e+00 4.42900346e-02 0.00000000e+00 3.84075551e-01]\n",
      " [5.57306603e-05 3.45388241e-05 4.79484018e-05 0.00000000e+00\n",
      "  5.39461917e-02 1.61838580e-01 3.87279525e-01 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v1')\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "# Write code for computing the state-value function\n",
    "v0 = np.zeros(env.observation_space.n) \n",
    "v = policy_evaluation(env, discount, agent, v0) \n",
    "print(v.reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_ip(env, discount, agent, v0, max_iter=10000, tol=1e-6):\n",
    "    \n",
    "    v = v0\n",
    "    \n",
    "    for i in range(max_iter): # Loop\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            vs = v[s]\n",
    "            \n",
    "            v[s] = Bellman_RHS(env, discount, agent, s, v) \n",
    "            \n",
    "            delta = np.max([delta, np.abs(vs-v[s])])\n",
    "            \n",
    "        if (delta < tol): # Until delta < tol\n",
    "            break\n",
    "            \n",
    "    return v    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(env, discount, agent, v):\n",
    "    \n",
    "    # The new policy will be a_probs\n",
    "    # We start by setting all probabilities to 0\n",
    "    # Then when we have found the greedy action in a state, \n",
    "    # we change the probability for that action to 1.0.\n",
    "    \n",
    "    a_probs = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    \n",
    "    for s in range(env.observation_space.n):\n",
    "        \n",
    "        action_values = np.zeros(env.action_space.n)\n",
    "        \n",
    "        for a in range(env.action_space.n):\n",
    "            # Compute action value for all actions\n",
    "            action_values[a] = compute_action_value(env, discount, s, a, v)\n",
    "            \n",
    "        a_max = np.argmax(action_values) # A greedy action\n",
    "        a_probs[s][a_max] = 1.0 # Always choose a greedy action!\n",
    "        \n",
    "    return a_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999935  0.99999455 0.99999585 0.99999711 0.99999821 0.99999907\n",
      "  0.99999967 1.        ]\n",
      " [0.99999327 0.99999406 0.99999522 0.99999648 0.99999765 0.99999863\n",
      "  0.9999994  1.        ]\n",
      " [0.99998212 0.97818546 0.92641739 0.         0.85661389 0.9462295\n",
      "  0.98207622 1.        ]\n",
      " [0.99997229 0.93458144 0.80107253 0.47489567 0.62361545 0.\n",
      "  0.94467658 1.        ]\n",
      " [0.99996407 0.82558801 0.5422199  0.         0.53933722 0.61118521\n",
      "  0.85195361 1.        ]\n",
      " [0.9999577  0.         0.         0.16803749 0.38321317 0.44226508\n",
      "  0.         1.        ]\n",
      " [0.99995335 0.         0.19466372 0.12090033 0.         0.33239888\n",
      "  0.         1.        ]\n",
      " [0.99995114 0.73152044 0.46309149 0.         0.27746574 0.5549319\n",
      "  0.77746574 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v1', render_mode='human')\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "# Enter code here\n",
    "v_old = np.zeros(env.observation_space.n) \n",
    "for i in range(10000): \n",
    "    v = policy_evaluation(env, discount, agent, v_old) \n",
    "    \n",
    "    if (np.max(np.abs(v-v_old))<1e-6): \n",
    "        break \n",
    "        \n",
    "    v_old = v \n",
    "    agent.probs = greedy_policy(env, discount, agent, v) \n",
    "\n",
    "print(v.reshape(8,8)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step: 200\n",
      "State: 15\n",
      "Action: 2\n",
      "Total reward: 0.0\n",
      "The environment was truncated even though a terminal state was not reached.\n"
     ]
    }
   ],
   "source": [
    "run_agent(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, discount, agent, v0, max_iter=10000, tol=1e-6):\n",
    "    \n",
    "    v = v0\n",
    "    \n",
    "    for i in range(max_iter): # Loop\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            vs = v[s]\n",
    "            \n",
    "            action_values = np.zeros(env.action_space.n) \n",
    "            \n",
    "            for a in range(env.action_space.n): \n",
    "                action_values[a] = compute_action_value(env, discount, s, a, v) \n",
    "            \n",
    "            v[s] = np.max(action_values) \n",
    "            \n",
    "            delta = np.max([delta, np.abs(vs-v[s])])\n",
    "            \n",
    "        if (delta < tol): # Until delta < tol\n",
    "            break\n",
    "            \n",
    "    print(i) \n",
    "            \n",
    "    return v    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575\n",
      "[[0.99998912 0.99998973 0.99999042 0.99999114 0.99999184 0.9999925\n",
      "  0.99999308 0.99999351]\n",
      " [0.99998918 0.99998967 0.99999029 0.99999098 0.99999168 0.99999237\n",
      "  0.99999308 0.99999391]\n",
      " [0.99997942 0.97818294 0.92641478 0.         0.85660997 0.94622427\n",
      "  0.98207059 0.99999454]\n",
      " [0.99997068 0.93458025 0.80107148 0.4748952  0.62361428 0.\n",
      "  0.94467234 0.99999539]\n",
      " [0.99996331 0.82558766 0.54221971 0.         0.53933775 0.61118505\n",
      "  0.85195124 0.9999964 ]\n",
      " [0.99995756 0.         0.         0.16803827 0.38321409 0.44226632\n",
      "  0.         0.99999754]\n",
      " [0.99995363 0.         0.19466435 0.12090087 0.         0.33239983\n",
      "  0.         0.99999876]\n",
      " [0.99995164 0.73152158 0.46309273 0.         0.27746659 0.5549332\n",
      "  0.7774666  0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.99998912, 0.99998973, 0.99999042, 0.99999114, 0.99999184,\n",
       "       0.9999925 , 0.99999308, 0.99999351, 0.99998918, 0.99998967,\n",
       "       0.99999029, 0.99999098, 0.99999168, 0.99999237, 0.99999308,\n",
       "       0.99999391, 0.99997942, 0.97818294, 0.92641478, 0.        ,\n",
       "       0.85660997, 0.94622427, 0.98207059, 0.99999454, 0.99997068,\n",
       "       0.93458025, 0.80107148, 0.4748952 , 0.62361428, 0.        ,\n",
       "       0.94467234, 0.99999539, 0.99996331, 0.82558766, 0.54221971,\n",
       "       0.        , 0.53933775, 0.61118505, 0.85195124, 0.9999964 ,\n",
       "       0.99995756, 0.        , 0.        , 0.16803827, 0.38321409,\n",
       "       0.44226632, 0.        , 0.99999754, 0.99995363, 0.        ,\n",
       "       0.19466435, 0.12090087, 0.        , 0.33239983, 0.        ,\n",
       "       0.99999876, 0.99995164, 0.73152158, 0.46309273, 0.        ,\n",
       "       0.27746659, 0.5549332 , 0.7774666 , 0.        ])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v1', render_mode=\"human\")\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "\n",
    "v0 = np.zeros(env.observation_space.n) \n",
    "v = value_iteration(env, discount, agent, v0) \n",
    "agent.probs = greedy_policy(env, discount, agent, v) \n",
    "print(v.reshape(8,8)) \n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step: 96\n",
      "State: 63\n",
      "Action: 2\n",
      "Total reward: 1.0\n",
      "A terminal state was reached.\n"
     ]
    }
   ],
   "source": [
    "run_agent(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
